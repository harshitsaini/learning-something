{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lggcAtkx4kMl"
   },
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T37JiXS0eh2R"
   },
   "source": [
    "### Torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "0NoRkDUApifl",
    "outputId": "743a6eda-91be-4836-b1bd-1abdbeea0f2a"
   },
   "outputs": [],
   "source": [
    "!pip -q install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
    "!pip -q install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OI1TCDY0ej6a"
   },
   "source": [
    "### Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HK2yYT9MzTw"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49853303/how-to-install-pydot-graphviz-on-google-colab?rq=1\n",
    "!pip -q install graphviz \n",
    "!apt-get install graphviz -qq\n",
    "!pip -q install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9n-MNinCtrf"
   },
   "outputs": [],
   "source": [
    "!pip -q install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ij7Z6h_HemCS"
   },
   "source": [
    "### Pycuda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYvXpk4pK_S-"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@iphoenix179/running-cuda-c-c-in-jupyter-or-how-to-run-nvcc-in-google-colab-663d33f53772\n",
    "# https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1704&target_type=deblocal\n",
    "!apt update -qq;\n",
    "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
    "!mv cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb  \n",
    "!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb\n",
    "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
    "!apt-get update -qq;\n",
    "!apt-get install cuda gcc-5 g++-5 -y -qq;\n",
    "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
    "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
    "#!apt install cuda-9.2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4n5e4qm836nz"
   },
   "outputs": [],
   "source": [
    "# http://alisonrowland.com/articles/installing-pycuda-via-pip\n",
    "# https://codeyarns.com/2015/07/31/pip-install-error-with-pycuda/\n",
    "import os\n",
    "PATH = os.environ[\"PATH\"]\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda-9.2/bin:/usr/local/cuda/bin:\" + PATH\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64\"\n",
    "os.environ[\"CUDA_ROOT\"] = \"/usr/local/cuda/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mpXHVIOKOBJ"
   },
   "outputs": [],
   "source": [
    "!pip -q install --ignore-installed pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKhW3jTB4m6u"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSfGRxu64oLZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, Process\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bEvBwSsoQfj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihLPzM0c40GO"
   },
   "source": [
    "# 1. Pool and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HqeJRinL5qUO",
    "outputId": "e2eaa0b1-19be-48ec-c8cd-31251781ce8d"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQO7_W7ftU3y"
   },
   "source": [
    "### Pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AryNxH0KvyJ"
   },
   "outputs": [],
   "source": [
    "def function(lst):\n",
    "  arr = np.zeros_like(lst)\n",
    "  for i in range(lst.shape[0]):\n",
    "    for j in range(lst.shape[1]):\n",
    "      arr[i][j] = lst[i][j] ** 2\n",
    "  return arr\n",
    "\n",
    "array = np.random.randint(1, 9, (2**10, 10000))\n",
    "data = np.array_split(array, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jEv7Wl9KG5Z0",
    "outputId": "7aa99406-795c-44b5-9d0a-a962d08a4793"
   },
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "qXAIPBpU4v2R",
    "outputId": "8e136df1-9b1b-4474-9350-066b78426a7c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with Pool(2) as p:\n",
    "  res = p.map(function, data)\n",
    "  p.close()\n",
    "  p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGvY7-IStXBk"
   },
   "source": [
    "### Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "3byO0zREtWyC",
    "outputId": "abe9f98a-4eed-4ca1-d7ca-dacb94aeb69d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processes = []\n",
    "for i in range(2):\n",
    "  p = Process(target=function, args=(data[i],))\n",
    "  processes.append(p)\n",
    "  p.start()\n",
    "  \n",
    "for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TGZQjmltPFm"
   },
   "source": [
    "### Graphing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPY_CnGOm_Y5"
   },
   "outputs": [],
   "source": [
    "def function(lst):\n",
    "  arr = np.zeros_like(lst)\n",
    "  for i in range(lst.shape[0]):\n",
    "      arr[i] = lst[i] ** 2\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41rY7dOwiWvq"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def serial(n, start=0):\n",
    "  times = []\n",
    "  size = []\n",
    "  for i in range(start, n):\n",
    "    s = 10**(i+1)\n",
    "    size.append(s)\n",
    "    lst = np.random.randint(1, 7, (s,))\n",
    "    st = time.time()\n",
    "    res = function(lst)\n",
    "    en = time.time()\n",
    "    times.append(en-st)\n",
    "  return times, size\n",
    "\n",
    "def parallel(n, sp, start=0):\n",
    "  times = []\n",
    "  size = []\n",
    "  for i in range(start, n):\n",
    "    s = 10**(i+1)\n",
    "    size.append(s)\n",
    "    lst = np.random.randint(1, 7, (s,))\n",
    "    splitted = np.split(lst, sp)\n",
    "    with Pool(sp) as p:\n",
    "      st = time.time()\n",
    "      res = p.map(function, splitted)\n",
    "      en = time.time()\n",
    "    times.append(en-st)\n",
    "  return times, size\n",
    "\n",
    "def parallel2(n, sp, start=0):\n",
    "  \"\"\" \n",
    "    sp: Number of splits of array and number of processes\n",
    "    start: Change it so that split function is able to split\n",
    "  \"\"\"\n",
    "  times = []\n",
    "  size = []\n",
    "  for i in range(start, n):\n",
    "    s = 10**(i+1)\n",
    "    size.append(s)\n",
    "    lst = np.random.randint(1, 7, (s,))\n",
    "    splitted = np.split(lst, sp)\n",
    "    processes = []\n",
    "    st = time.time()\n",
    "    for j in range(sp):\n",
    "      p = Process(target=function, args=(splitted[j],))\n",
    "      processes.append(p)\n",
    "    for p in processes: p.start()\n",
    "    for p in processes: p.join()\n",
    "    en = time.time()\n",
    "    times.append(en-st)\n",
    "  return times, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "xhIPLL_jjrzC",
    "outputId": "07806d10-ff68-4d9b-f46f-5e3a8042f708"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "t1, s1 = serial(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "hPhVPrwtyPS_",
    "outputId": "21bb9cbe-0ce0-4d95-8fea-bd1f0f1ae15a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "t2, s2 = parallel(6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "xDbYdL8tyROL",
    "outputId": "766b202c-6ce7-4423-aaf0-6789402a96a4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "t3, s3 = parallel2(6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Hy2kqrLGkEDD",
    "outputId": "379bbe2b-08cb-49da-b55d-d128cedd5c7e"
   },
   "outputs": [],
   "source": [
    "plt.plot(s1, t1, \"o-\", label=\"Serial\")\n",
    "plt.plot(s2, t2, \"o-\", label=\"Pool\")\n",
    "plt.plot(s3, t3, \"o-\", label=\"Process\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of elements:\")\n",
    "plt.ylabel(\"Time (sec):\")\n",
    "plt.show()\n",
    "# Our task is not that heavy, results here may vary unexpectedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeV5R2MIUl9I"
   },
   "source": [
    "# 2. Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DU9mgOzMSD2a"
   },
   "outputs": [],
   "source": [
    "from threading import Thread as trd\n",
    "import queue\n",
    "q = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoJNA8wBYR5j"
   },
   "outputs": [],
   "source": [
    "def function(lst):\n",
    "  arr = np.zeros_like(lst)\n",
    "  for i in range(lst.shape[0]):\n",
    "    for j in range(lst.shape[0]):\n",
    "      arr[i][j] = lst[i][j] * lst[i][j]\n",
    "  return arr\n",
    "\n",
    "array = np.random.randint(1, 10, (1000, 10000))\n",
    "data = np.array_split(array, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "iR6_Vx8yB1hB",
    "outputId": "c84844cc-95c2-4835-9174-1db62309036e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = function(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rgze9tLExmcB",
    "outputId": "be30edd7-c227-44c3-be4a-47e37b3f3d89"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# By using Queue this way you can get result of function without\n",
    "# modifying your function.\n",
    "t1 = trd(target=lambda q, args1: q.put(function(args1)), args=(q, data[0]))\n",
    "t2 = trd(target=lambda q, args1: q.put(function(args1)), args=(q, data[1]))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "res1 = q.get()\n",
    "res2 = q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l4JoToF23vMq",
    "outputId": "d3ab3029-3a16-4bda-d0e5-21de50da1641"
   },
   "outputs": [],
   "source": [
    "q.empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZJAmma8Hly4"
   },
   "source": [
    "# 3. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKPgNyg1Hovh"
   },
   "outputs": [],
   "source": [
    "from dask import delayed as delay\n",
    "\n",
    "@delay\n",
    "def add(x, y): return x+y\n",
    "@delay\n",
    "def sq(x): return x**2\n",
    "@delay\n",
    "def sum(x): \n",
    "  sum=0\n",
    "  for i in range(len(x)): sum+=x[i]\n",
    "  return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5zzu_Z3HoS9"
   },
   "outputs": [],
   "source": [
    "inputs = list(np.arange(1, 11))\n",
    "\n",
    "res = [sq(n) for n in inputs]\n",
    "res = [add(n, m) for n, m in zip(res[::2], res[1::2])]\n",
    "res = sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "colab_type": "code",
    "id": "O84kIMvIGnMA",
    "outputId": "de6d650b-6b0a-47cb-b7e1-041100499b6a"
   },
   "outputs": [],
   "source": [
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gMZ5FXaKNMVJ",
    "outputId": "e9b41afb-29d1-4830-b6d9-9f8738b34f92"
   },
   "outputs": [],
   "source": [
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THHQmgFGJuwj"
   },
   "source": [
    "# 4. torch.multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JV33jJ274CEr"
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp_\n",
    "mp = mp_.get_context('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdQ3-4RDpXVp"
   },
   "outputs": [],
   "source": [
    "a = torch.zeros((1000, 1000))\n",
    "b = torch.zeros_like(a).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a51bUqYLM00r"
   },
   "outputs": [],
   "source": [
    "def func(arr):\n",
    "  for i in range(arr.shape[0]):\n",
    "    for j in range(arr.shape[1]):\n",
    "      arr[i][j] += (i+j)\n",
    "      arr[i][j] *= arr[i][j]\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "uetR__ZHVvRy",
    "outputId": "4bd112df-4595-40ed-d624-b6d77054c9bc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = func(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Eq7Q1ionpeuM",
    "outputId": "48d3b1c3-797d-42e4-9b08-17a1158ff1db"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = func(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jui24mDtNU5B"
   },
   "source": [
    "### Training Model using multiple processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ep4CbC9NTkr"
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp_\n",
    "mp = mp_.get_context('spawn')\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBrMPv2YPDDX"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "X, y = make_classification(n_samples=10000, )\n",
    "\n",
    "dataset = TensorDataset(torch.FloatTensor(X), torch.DoubleTensor(y))\n",
    "data_loader = DataLoader(dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "z00bFYOMNsWL",
    "outputId": "8dca63bc-7db5-418a-d55e-52467b49006e"
   },
   "outputs": [],
   "source": [
    "n_in = 20; n_out = 1        \n",
    "        \n",
    "model = nn.Sequential(nn.Linear(n_in, 15),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(15, 10),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(10, 5),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(5, n_out),\n",
    "                      nn.Sigmoid())\n",
    "\n",
    "model.share_memory() # Required for 'fork' method to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0jUB8wJFJXq"
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    for data, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss_fn(model(data), labels).backward()\n",
    "        optimizer.step()  # This will update the shared parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcR9XR7fOLFJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SlQx-zCN92f"
   },
   "outputs": [],
   "source": [
    "processes = []\n",
    "for i in range(4): # No. of processes\n",
    "    p = mp.Process(target=train, args=(model,))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l2Kmb16sCehz",
    "outputId": "318f7681-306f-4ff0-cc9f-3c8f6a06b54e"
   },
   "outputs": [],
   "source": [
    "sum=0\n",
    "for data, labels in data_loader:\n",
    "  with torch.no_grad():\n",
    "    res = model(data)\n",
    "    res[res>=0.7] = 1\n",
    "    res[res<0.7] = 0\n",
    "    sum += (res.numpy()!=labels.float().numpy()).diagonal().sum()\n",
    "\n",
    "    \n",
    "sum/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7435zkZpRyj"
   },
   "source": [
    "# 5. Pycuda (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvcFFniYahLl"
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAMWyy3xyT72"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = numpy.random.randint(1, 9, (10000, 10000))\n",
    "a = a.astype(numpy.float32) # Most nVidia devices support single precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuMJGXOTzDtn"
   },
   "outputs": [],
   "source": [
    "# The main drawback of Pycuda is that you will have to write C code\n",
    "# to perform any task and pass it to SourceModule...\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void double(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*blockDim.x;\n",
    "    a[idx] *= 2;\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "# As we saw in Numba post, you have to know which thread we are in to compute index(s) to operate on.\n",
    "# Here we get that from threadId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYVDamqZ10gu"
   },
   "outputs": [],
   "source": [
    "# Get reference to function and initialize it, passing array and block size (4x4).\n",
    "func = mod.get_function(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqXL4t-V2Ah4"
   },
   "outputs": [],
   "source": [
    "## a_gpu = cuda.mem_alloc(a.nbytes) # Allocate memory on GPU\n",
    "## cuda.memcpy_htod(a_gpu, a) # Send array to GPU. Now your array is within variable a_gpu\n",
    "### a_gpu = cuda.to_device(a)\n",
    "### func(a_gpu, block=(4, 4, 1))\n",
    "### a_doubled = numpy.empty_like(a) # Copy array from gpu to cpu\n",
    "### cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "### a_doubled[0][0:10], a[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AaNr4gON2Tuh"
   },
   "outputs": [],
   "source": [
    "# We can also simply use cuda.InOut which sends array to GPU\n",
    "# and then retrives back again:\n",
    "func(cuda.InOut(a), block=(4, 4, 1)) # block takes Block size which can be 3D. Block size gives us number of threads in Block. (No. of threads should be <= 512)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jG7mqYikv-0c"
   },
   "outputs": [],
   "source": [
    "# Whereas a grid is 2D and can have max of 1000's of blocks. As before its dimensions give number of blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORmOJq4MvKib"
   },
   "source": [
    "#### Examples:\n",
    "\n",
    "Image: https://raw.githubusercontent.com/andreajeka/CUDAThreadIndexing/master/images/1dgrid3dblock.png\n",
    "\n",
    "\n",
    "As shown in image, if we use 1D grid and 3D blocks, that is how you can think of it. Each of the thread sub-blocks contain one thread. Structure like this makes indexing of threads in grid easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "sZbfoqjveSbV",
    "outputId": "ff0ece66-1a00-4fd2-fc6c-68afc4dbfabd"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 1D grid and 1D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a)\n",
    "  {\n",
    "    int idx = blockDim.x*blockIdx.x + threadIdx.x;  // Go to block + Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (4127)//128 = 32 blocks, each having 128 threads. Total 4096 threads.\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), grid=((4000+127)//128, 1), block=(128, 1, 1))\n",
    "temp[-33:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ZVvC9b6Njh2K",
    "outputId": "c1b7e6f7-2046-46a3-ad0a-dc4d47542a0a"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 1D grid and 2D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp2 = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "temp2 = temp2.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a, float *b)\n",
    "  {\n",
    "    int idx = blockDim.x*blockDim.y*blockIdx.x + blockDim.x* threadIdx.y + threadIdx.x;  // Go to block + Go to row to current thread + Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (4063)//64 = 63 blocks, each having (8x8)=64 threads. Total 4032 threads.\n",
    "       b[idx] = (float) threadIdx.y;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), cuda.InOut(temp2), grid=((4000+63)//64, 1), block=(8, 8, 1))\n",
    "temp[-10:], temp2[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "fyUsLwBJoH4d",
    "outputId": "30827c24-a3ff-44f7-c895-3313df44ae06"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 1D grid and 3D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "temp2 = numpy.zeros((4000,))\n",
    "temp2 = temp2.astype(numpy.float32)\n",
    "temp3 = numpy.zeros((4000,))\n",
    "temp3 = temp3.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a, float *b, float *c)\n",
    "  {\n",
    "    int idx = blockDim.x*blockDim.y*blockDim.z*blockIdx.x +  // Go to block \n",
    "              blockDim.x*blockDim.y*threadIdx.z +            // Go to z sclice containing thread\n",
    "              blockDim.x*threadIdx.y +                       // In that slice go to row containing thread\n",
    "              threadIdx.x;                                   // Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (4063)//64 = 63 blocks, each having (4x4x4)=64 threads. Total 4032 threads.\n",
    "       b[idx] = (float) threadIdx.y;\n",
    "       c[idx] = (float) threadIdx.z;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), cuda.InOut(temp2), cuda.InOut(temp3), grid=((4000+63)//64, 1), block=(4, 4, 4))\n",
    "temp[-17:], temp2[-17:], temp3[-17:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "BAqVLfRRh5nL",
    "outputId": "734bdc99-9986-47fc-dc0c-713bae2c8e93"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 2D grid and 3D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "temp2 = numpy.zeros((4000,))\n",
    "temp2 = temp2.astype(numpy.float32)\n",
    "temp3 = numpy.zeros((4000,))\n",
    "temp3 = temp3.astype(numpy.float32)\n",
    "temp4 = numpy.zeros((4000,))\n",
    "temp4 = temp4.astype(numpy.float32)\n",
    "temp5 = numpy.zeros((4000,))\n",
    "temp5 = temp5.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a, float *b, float *c, float *d, float *e)\n",
    "  {\n",
    "    int idx = gridDim.x*blockDim.x*blockDim.y*blockDim.z*blockIdx.y +      // Go to grid row containing block\n",
    "              blockDim.x*blockDim.y*blockDim.z*blockIdx.x +                // Go to block\n",
    "              blockDim.x*blockDim.y*threadIdx.z +                          // Go to slice of block containing thread\n",
    "              blockDim.x*threadIdx.y +                                     // In that slice go to row containing thread\n",
    "              threadIdx.x;                                                 // Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (8*8) = 64 blocks, each having (4x4x4)=64 threads. Total 4096 threads.\n",
    "       b[idx] = (float) threadIdx.y;\n",
    "       c[idx] = (float) threadIdx.z;\n",
    "       d[idx] = (float) blockIdx.x;\n",
    "       e[idx] = (float) blockIdx.y;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), cuda.InOut(temp2), cuda.InOut(temp3), cuda.InOut(temp4), cuda.InOut(temp5), grid=(8, 8), block=(4, 4, 4))\n",
    "temp[-17:], temp2[-17:], temp3[-17:], temp4[-17:], temp5[-17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "SmlbQLcq3VDy",
    "outputId": "075eb450-fe7c-413d-db8f-6c16a483137f"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 1D grid and 1D block and only 1 thread\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a)\n",
    "  {\n",
    "    printf(\"%d\", threadIdx.x);\n",
    "    for(int i=0; i<4000; i++){\n",
    "      a[i] = i;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), grid=(1, 1), block=(1, 1, 1))\n",
    "temp[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9KBHlvd4Gk9"
   },
   "outputs": [],
   "source": [
    "# So on a single thread you can operate on an subarray too. You will have to configure task accordingly."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lggcAtkx4kMl",
    "T37JiXS0eh2R",
    "OI1TCDY0ej6a",
    "Ij7Z6h_HemCS",
    "UKhW3jTB4m6u",
    "ihLPzM0c40GO",
    "UeV5R2MIUl9I",
    "fZJAmma8Hly4",
    "THHQmgFGJuwj",
    "Jui24mDtNU5B",
    "v7435zkZpRyj",
    "ORmOJq4MvKib"
   ],
   "name": "SpeedUpYourAlgo-Prallelization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
