{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# The code performs logistic/softmax regression on the MNIST Dataset\n",
    "# It is based upon the tutorial given at Tensorflow.org : https://www.tensorflow.org/get_started/mnist/beginners\n",
    "# We have added few Tensorboard summaries to understand it better\n",
    "\n",
    "# Import Modules needed\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt,  matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Input \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyper Parameters of the model:\n",
    "learning_rate = 0.05\n",
    "batch_size = 100\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/tensorflow/lib/python3.6/site-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "i = 55\n",
    "img = mnist.train.images[i]\n",
    "img=img.reshape((28,28))\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.title(mnist.train.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow graph inputs\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='X')\n",
    "y = tf.placeholder(tf.float32, [None, 10],name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Variables\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='W')\n",
    "b = tf.Variable(tf.zeros([10]), name='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "with tf.name_scope(\"wx_b\") as scope:\n",
    "    y_hat = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add summary ops to collect data while training\n",
    "w_h = tf.summary.histogram(\"weights\", W)\n",
    "b_h = tf.summary.histogram(\"biases\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-entropy loss function\n",
    "with tf.name_scope('cross-entropy') as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat))\n",
    "    tf.summary.scalar('cross-entropy', loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimizer\n",
    "with tf.name_scope('Train') as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ops to test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge All summaries\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 2.2506228308244185\n",
      "Epoch 1: Loss 2.0905995193394746\n",
      "Epoch 2: Loss 1.9606654973463578\n",
      "Epoch 3: Loss 1.8827280853011392\n",
      "Epoch 4: Loss 1.833503966548226\n",
      "Epoch 5: Loss 1.8015771189602938\n",
      "Epoch 6: Loss 1.779589732993733\n",
      "Epoch 7: Loss 1.763399433439428\n",
      "Epoch 8: Loss 1.7509475094621831\n",
      "Epoch 9: Loss 1.740984851663763\n",
      "Epoch 10: Loss 1.7328140406175094\n",
      "Epoch 11: Loss 1.7259703946113587\n",
      "Epoch 12: Loss 1.7201126764037393\n",
      "Epoch 13: Loss 1.715042025826194\n",
      "Epoch 14: Loss 1.7105936440554532\n",
      "Epoch 15: Loss 1.706652053486217\n",
      "Epoch 16: Loss 1.7031095372546803\n",
      "Epoch 17: Loss 1.6999168580228632\n",
      "Epoch 18: Loss 1.6970095012404702\n",
      "Epoch 19: Loss 1.6943312456391075\n",
      "Epoch 20: Loss 1.6918623237176376\n",
      "Epoch 21: Loss 1.689540527083657\n",
      "Epoch 22: Loss 1.6873508301648226\n",
      "Epoch 23: Loss 1.685213943611492\n",
      "Epoch 24: Loss 1.6830464107340033\n",
      "Epoch 25: Loss 1.6806079968539152\n",
      "Epoch 26: Loss 1.677191168828444\n",
      "Epoch 27: Loss 1.6712743730978532\n",
      "Epoch 28: Loss 1.6643862878192555\n",
      "Epoch 29: Loss 1.6593245534463363\n",
      "Epoch 30: Loss 1.6548790946873753\n",
      "Epoch 31: Loss 1.6506461915102872\n",
      "Epoch 32: Loss 1.6465965526754205\n",
      "Epoch 33: Loss 1.642783284404061\n",
      "Epoch 34: Loss 1.6392241495305842\n",
      "Epoch 35: Loss 1.6359832113439388\n",
      "Epoch 36: Loss 1.633065214807337\n",
      "Epoch 37: Loss 1.630462630445307\n",
      "Epoch 38: Loss 1.6281436902826483\n",
      "Epoch 39: Loss 1.6260701272704385\n",
      "Epoch 40: Loss 1.6241916357387196\n",
      "Epoch 41: Loss 1.622474571358074\n",
      "Epoch 42: Loss 1.6208947743069042\n",
      "Epoch 43: Loss 1.6194298512285406\n",
      "Epoch 44: Loss 1.6180609661882575\n",
      "Epoch 45: Loss 1.6167725612900474\n",
      "Epoch 46: Loss 1.615547557744113\n",
      "Epoch 47: Loss 1.6143988849899986\n",
      "Epoch 48: Loss 1.6133026959679344\n",
      "Epoch 49: Loss 1.6122572454539212\n",
      "Epoch 50: Loss 1.6112623678554188\n",
      "Epoch 51: Loss 1.6103046777031638\n",
      "Epoch 52: Loss 1.6093888232924722\n",
      "Epoch 53: Loss 1.6085031873529607\n",
      "Epoch 54: Loss 1.6076605517213995\n",
      "Epoch 55: Loss 1.6068435968052257\n",
      "Epoch 56: Loss 1.60605243769559\n",
      "Epoch 57: Loss 1.6052946944670243\n",
      "Epoch 58: Loss 1.604559862830422\n",
      "Epoch 59: Loss 1.603848756876859\n",
      "Epoch 60: Loss 1.6031589629433372\n",
      "Epoch 61: Loss 1.6024917231906544\n",
      "Epoch 62: Loss 1.6018422668630428\n",
      "Epoch 63: Loss 1.6012171918695624\n",
      "Epoch 64: Loss 1.600604227889668\n",
      "Epoch 65: Loss 1.6000111423839223\n",
      "Epoch 66: Loss 1.5994328357956626\n",
      "Epoch 67: Loss 1.5988670366460627\n",
      "Epoch 68: Loss 1.5983236755024304\n",
      "Epoch 69: Loss 1.597778577154333\n",
      "Epoch 70: Loss 1.5972746233506636\n",
      "Epoch 71: Loss 1.596761554371227\n",
      "Epoch 72: Loss 1.5962653116746381\n",
      "Epoch 73: Loss 1.595779107050462\n",
      "Epoch 74: Loss 1.5953113527731462\n",
      "Epoch 75: Loss 1.5948456870425831\n",
      "Epoch 76: Loss 1.5943977704915133\n",
      "Epoch 77: Loss 1.5939532455531034\n",
      "Epoch 78: Loss 1.593522161353718\n",
      "Epoch 79: Loss 1.5930991627953268\n",
      "Epoch 80: Loss 1.5926831325617703\n",
      "Epoch 81: Loss 1.5922743914344093\n",
      "Epoch 82: Loss 1.5918848930705678\n",
      "Epoch 83: Loss 1.5914929197051308\n",
      "Epoch 84: Loss 1.5911110990697688\n",
      "Epoch 85: Loss 1.5907347507910294\n",
      "Epoch 86: Loss 1.5903678839856927\n",
      "Epoch 87: Loss 1.5900084075060759\n",
      "Epoch 88: Loss 1.5896590120142156\n",
      "Epoch 89: Loss 1.5893091895363547\n",
      "Epoch 90: Loss 1.5889685726165772\n",
      "Epoch 91: Loss 1.5886335509473628\n",
      "Epoch 92: Loss 1.5883031118999829\n",
      "Epoch 93: Loss 1.5879796784574336\n",
      "Epoch 94: Loss 1.5876629740541632\n",
      "Epoch 95: Loss 1.5873504614830016\n",
      "Epoch 96: Loss 1.5870425564592534\n",
      "Epoch 97: Loss 1.5867417818849736\n",
      "Epoch 98: Loss 1.5864429369839754\n",
      "Epoch 99: Loss 1.5861510701612993\n",
      "Done\n",
      "0.9089\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)  # initialize all variables\n",
    "    summary_writer = tf.summary.FileWriter('graphs', sess.graph)  # Create an event file\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(max_epochs):\n",
    "        loss_avg = 0\n",
    "        num_of_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(num_of_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)  # get the next batch of data\n",
    "            _, l, summary_str = sess.run([optimizer,loss, merged_summary_op], feed_dict={x: batch_xs, y: batch_ys})  # Run the optimizer\n",
    "            loss_avg += l\n",
    "            summary_writer.add_summary(summary_str, epoch*num_of_batch + i)  # Add all summaries per batch\n",
    "            \n",
    "        loss_avg = loss_avg/num_of_batch\n",
    "        print('Epoch {0}: Loss {1}'.format(epoch, loss_avg))\n",
    "    \n",
    "    print('Done')\n",
    "    \n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images,y: mnist.test.labels}))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
